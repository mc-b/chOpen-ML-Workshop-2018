{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook shortcuts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Esc: go to command mode\n",
    "* Enter: go to edit mode\n",
    "\n",
    "\n",
    "* m: switch cell to markdown\n",
    "* y: switch cell to code\n",
    "\n",
    "\n",
    "* Shift-Enter: Execute cell\n",
    "* dd: delete cell\n",
    "\n",
    "\n",
    "* a: Insert new cell above\n",
    "* b: Insert new cell below\n",
    "\n",
    "\n",
    "* k: Move to cell above\n",
    "* j: Move to cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a code cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is a markdown cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Follow along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning - Typischer Ablauf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fragestellung definieren\n",
    "1. Metrik festlegen\n",
    "1. Daten beschaffen\n",
    "1. Explorative Datenanalyse\n",
    "1. Daten aufbereiten\n",
    " 1. Tidy Data\n",
    " 1. Fehler bereinigen\n",
    " 1. Null-Werte behandeln\n",
    " 1. Ausreisser behandeln\n",
    " 1. Kategorische Variablen behandeln\n",
    "1. Feature Engineering\n",
    " 1. Feature Extraction\n",
    " 1. Feature Construction\n",
    " 1. Feature Selection\n",
    "1. Validierungsmethodik festlegen\n",
    "1. Training (Model building)\n",
    " 1. Modelle trainieren\n",
    " 1. Hyperparameter optimieren\n",
    " 1. Validieren\n",
    "1. Ensembling\n",
    "1. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fragestellung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Welchen Preis erzielen meine Immobilien?\n",
    "* Ziel: Voraussage des Verkaufspreises von Immobilien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ziel:\n",
    "  * Möglichst geringe Abweichung zwischen wirklichem und vorausgesagtem Preis\n",
    "  * Abweichungen gegen oben oder unten sind \"gleich schlecht\", d.h. es ist egal, ob wir uns gegen oben oder gegen unten verschätzen, nur auf den Betrag der Abweichung kommt es an.\n",
    "* Gewählte Metrik:\n",
    "  * Root Mean Squared Error: $\\textrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y^{(i)} - \\hat{y}^{(i)})^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorie-Einschub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beim Trainieren von Supervised Learning Algorithmen gibt es zwei Quellen von Fehlern, die dem erfolgreichen Generalisieren über das Trainingsset hinweg entgegenwirken:\n",
    "\n",
    "* Bias: Unterschied zwischen effektiven (gemessenen) Wert und vom Algorithmus vorhergesagten Wert.\n",
    "* Varianz: Sensitivität gegenüber Noise im Trainingsset\n",
    "\n",
    "Wenn das eine sinkt, steigt normalerweise das andere. Das Ziel ist immer, beide zusammen zu optimieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/bias_variance.png\" height=\"40%\" width=\"40%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hohe Varianz, tiefer Bias (\"Modell ist zu komplex und bildet die Daten zu genau ab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hoher Bias, tiefe Varianz (\"Modell ist zu simpel, um die Daten abbilden zu können\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten beschaffen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für dieses Beispiel verwenden wir das *Ames Housing Dataset*, welches Immobilieneigenschaften und Verkauspreise von Häusern aus Iowa beinhaltet.\n",
    "* Paper: http://www.amstat.org/publications/jse/v19n3/decock.pdf\n",
    "* Daten: https://ww2.amstat.org/publications/jse/v19n3/decock/AmesHousing.txt\n",
    "* Codebook: https://ww2.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 3 AmesHousing.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"AmesHousing.txt\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nur damit Beispiel mit Dummies weiter unten anschaulicher wird\n",
    "df = df.sample(frac=1, random_state=16).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um das Beispiel etwas übersichtlicher zu gestalten, behalten wir nur einige wenige Spalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns = ['Lot Area', 'Bldg Type', 'Overall Qual', 'Overall Cond',\n",
    "                'Year Built', 'Year Remod/Add', 'TotRms AbvGrd', 'Garage Area', 'SalePrice']\n",
    "df = df[keep_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorative Datenanalyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Statistische Verteilung der Daten\n",
    "* Class imbalance der Targetvariable (oder Skew bei einem Regressionsproblem)\n",
    "* Plots, Histogramme, Summary Statistics\n",
    "* Korrelationen der Features untereinander und zum Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='box', subplots=True, sharex=False, layout=(2,4), figsize=(18,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='kde', subplots=True, sharex=False, layout=(2,4), figsize=(18,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten Aufbereiten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidy Data Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Eine Variable pro Spalte (1NF)\n",
    "1. Eine Beobachtung pro Zeile\n",
    "1. Einen header mit verständlichen Bezeichnern\n",
    "1. Eine Tabelle pro Art der Beobachtung\n",
    "1. Eine Methode zum joinen von Tabellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punkte 1, 2 sind bereits erfüllt, 4 und 5 nicht relevant, da wir nur eine Tabelle haben. Wir passen lediglich noch die Spaltennamen an, um sie etwas verständlicher zu machen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['grdstkfläche (k)', 'haustyp (n)', 'qualität (o)', 'zustand (o)', 'baujahr (d)',\n",
    "             'renovationsjahr (d)', 'zimmer (d)', 'garagenfläche (k)', 'PREIS']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fehler bereinigen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tippfehler\n",
    "* Unterschiedliche Schreibweisen\n",
    "* Weitere Fehler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null-Werte behandeln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gründe für Null-Werte herausfinden\n",
    " * Programmierfehler\n",
    " * Messfehler, korrupter Wert\n",
    " * Techn. Problem (z.B. Sensorausfall)\n",
    " * Nichtmessbarkeit (z.B. Wert bei entsprechendem Element nicht relevant)\n",
    " * Null-Wert bedeutet 0, False oder Absenz\n",
    " \n",
    " \n",
    "* Null-Werte zuerst als np.NaN kodieren\n",
    "\n",
    "\n",
    "* Was machen wir danach mit Null-Werten?\n",
    " * Zeilen mit Nullwerten weglassen (wenn es nur wenige sind)\n",
    " * Spalten mit Nullwerten weglassen (wenn es zuviele sind)\n",
    " * Imputation (fehlende Werte ableiten):\n",
    "   * Mittelwert, Median, häufigster Wert einer Spalte wählen\n",
    "   * Wert der vorherigen oder nachfolgender Zeile verwenden (wenn es eine Reihenfolge in den Samples gibt)\n",
    "   * Fehlender Wert aus anderen Werten vorhersagen, z.B. mit Regression oder Clustering\n",
    " * So lassen (einige Algorithem können damit umgehen, z.B. XGboost)\n",
    " * Die Information \"Viele Nullwerte\" als neuer Feature verwenden (siehe Feature Engineering weiter unten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Ausreisser behandeln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wie bei Null-Werten zuerst herausfinden, ob es einen legitimen Grund für Ausreisser gibt\n",
    "* Ausreisser finden ist nicht ganz einfach\n",
    "* Grundsätzlich zwei Ansätze:\n",
    "  * Parametrisch: Anname: Daten folgen einer bestimmten Verteilung, dann ist es einfach zu definieren, wie weit weg vom Median ein Datenpunkt liegen muss, um als Ausreisser betrachtet werden zu können\n",
    "  * Nicht-Parametrisch: Ohne diese Annahmem, z.B. mittels Clustering\n",
    "* Verbleibende Ausreisser beinflussen die spätere Wahl des Prediction Algorithmus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kategorische Variablen behandeln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Die Algorithmen brauchen numerischen Input\n",
    "* Im Beispiel haben wir aber eine Spalte mit Chars\n",
    "\n",
    "\n",
    "Allgemein werden bei statistischen Daten die Spalten in zwei Typen mit je zwei Untertypen eingeteilt:\n",
    "* Numerisch: messbare Grösse, Zählwert\n",
    " * diskret (z.B. 1,2,3,4,5)\n",
    " * kontinuierlich (z.B. 12.152, 17.882, 20.5)\n",
    "* Categorical: Ausprägung\n",
    " * nominal: ohne natürliche Ordnung, z.B. Geschlecht, Land\n",
    " * ordinal: mit Ordnung bzw. Rang, z.B. Sternebewertung bei Hotels, T-Shirt Grössen\n",
    "\n",
    "\n",
    "Behandlung:\n",
    "* Für ordinale Werte: enumeration\n",
    "* Für nominale Werte: One-Hot (bei Trees sind enums auch OK)\n",
    "* [Hashing Trick](https://en.wikipedia.org/wiki/Feature_hashing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['haustyp (n)'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['haustyp (n)'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set von Features bereitstellen\n",
    "* Benötigt Domainwissen, Statistik-Knowhow und Erfahrung\n",
    "* Grosser Impact auf Performance\n",
    "* Neuronale Netze machen dies selber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Automatisches Generieren von Features aus Rohdaten\n",
    "* Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Manuelles generieren mit Domain- und Statistikwissen\n",
    "* Aggregieren, kombinieren, Logarithmus, Anscombe, TF-IDF usw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Features mit viel Predictive Power auswählen\n",
    "* Redundante und irrelevante Features entfernen\n",
    "* Anzahl verringern: Die Anzahl bestimmt die Parameter, die gelernt werden müssen. Je weniger, desto geringer die Gefahr von Overfitting (einfacheres Modell) und desto weniger Trainingsdaten werden benötigt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man könnte sich zum Beispiel überlegen, dass der Preis eines Hauses in Abhängigkeit zum Renovationsjahr nicht linear zunimmt. Erst vor kurzem gemachte Renovationen bringen viel für den Preis, länger zurückliegende jedoch kaum was.\n",
    "\n",
    "Somit könnte man aus dem Renovationsjahr eine \"Zeit seit der letzten Renovation machen\" und von dieser den Logarithmus nehmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['zeitseitrenovation'] = max(df['renovationsjahr (d)'])-df['renovationsjahr (d)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_renovation (n)'] = np.log(df['zeitseitrenovation']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['renovationsjahr (d)', 'zeitseitrenovation'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validierungsmethodik festlegen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wir validieren immer mit Samples, die der Algorithmus wärend des Trainings nicht gesehen hat\n",
    "* Um einen Modell zu trainieren und anschliessend zu validieren, werden die Trainingsdaten deshalb aufgeteilt\n",
    "* Einfache Stragegie: Zwei Teile, ca 80% zum Trainieren und 20% zum Validieren\n",
    "* Wie die Daten aufgeteilt werden, ist nicht immer offensichtlich (shuffle, stratification, time series)\n",
    "\n",
    "\n",
    "* Achtung bei der Begrifflichkeit:\n",
    "  * Meist wird in Beispielen das mit Labels versehene Trainingsset in *train* und *test* aufgeteilt, und test zum validieren verwendet\n",
    "  * Manchmal (und das ist eigentlich die bessere Terminologie) wird das mit Labels versehene Trainingsset in *train* und *validation* aufgeteilt, während man davon ausgeht, dass für das Testset keine Labels vorhanden sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir teilen unser Trainingsset in zwei Teile, 80% davon verwenden wir für das Training unseres Algorithmus und 20% legen wir zur Seite, um nach erfolgtem Training überprüfen zu können, wie unsere trainierten Algorithmen abschneiden.\n",
    "\n",
    "Zudem lagern wir das Target (den Preis) in eine eigene Variable aus. Dies, weil die von uns verwendete Machine Learning Bibliothek [Scikit-Learn (sklearn)](http://scikit-learn.org/) das so haben möchte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['PREIS']\n",
    "df.drop(['PREIS'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun verwenden wir zum ersten Mal in diesem Beispiel die Python Machine Learning Bilbiothek [Scikit-Learn](http://scikit-learn.org/stable). Wir verwenden die Funktion train_test_split auf unseren Dataframe mit den Trainingsdaten und der Variable y mit der Targetvariable, und erhalten 4 Rückgabewerte:\n",
    " * X_train (Features zum Trainieren)\n",
    " * y_train (Target zum Trainieren, wir machen hier ja Supervised Learning)\n",
    " * X_test (Features zum validieren, legen wir zur Seite)\n",
    " * y_test (Target zum validieren, legen wir zur Seite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.shape, X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (Model building)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun folgt ein iterativer Schritt, bei dem verschiedene Algorithmen mit verschiedenen Parametern trainiert und die Resultate verglichen werden.\n",
    "\n",
    "Natürlich ist immer auch der gesamte Prozess iterativ, man geht immer wieder auch zurück zur Explorativen Datenanalyse und zum Feature Engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell trainieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wählen im Beispiel nun zwei Algorithmen. Einmal Linear least squares (Lineare Regression) mit L2 Regularization und einmal Random Forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "lr = linear_model.Ridge(alpha=.5, normalize=True)\n",
    "lr.fit(X_train, y_train) # lr ist nun unser trainiertes Model (Linear Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=500, n_jobs=-1, random_state=72)\n",
    "rf.fit(X_train, y_train) # rf ist nun unser trainiertes Model (Random Forests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Hyperparameter werden diejenigen Parameter bezeichnet, welche beeinflussen, wie der Algorithmus sich beim Training verhält. Die \"nicht-hyper-parameter\" sind diejenigen, die der Algorithmus durch das Training ermittelt (also die Parameter des trainierten Modells).\n",
    "\n",
    "Im obigen Beispiel sind dies alpha, normalize und n_estimators (n_jobs gibt lediglich die Anzahl der zu verwendenden CPU Cores an, random_state seedet den Zufallszahlengenerator).\n",
    "\n",
    "Hyperparameter haben einen grossen Einfluss auf die Performanz eines Algorithmus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haben wir einen Algorithmus trainiert, möchten wir nun wissen, wie gut er in Bezug auf unsere oben definierte Metrik, dem Root Mean Squared Error $\\textrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y^{(i)} - \\hat{y}^{(i)})^2}$ abschneidet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zuerst verwenden wir nun unsere beiden trainierten Modelle, und machen Predictions für das Test Set (die 20%, welche wir oben zur Seite gelegt haben)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr = lr.predict(X_test) # Linear Regression\n",
    "pred_rf = rf.predict(X_test) # Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pred_lr und pred_rf enthalten nun die Immobilienpreise, die unser Algorithmus für die 20% des zur Seite gelegten Testsets voraussagt. Wir vergleichen diese Voraussagen nun mit den tatsächlichen Verkaufspreisen. Als Mass verwenden wir unseren RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, pred_lr))\n",
    "rmse_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, pred_rf))\n",
    "rmse_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit dem RMSE können wir unsere zwei Modelle vergleichen, und sehen, dass Random Forests besser als Lineare Regression ist (wir messen den Error, je kleiner der Wert desto besser also)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Einschub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Eine etwas aufwändigere Strategie zum Validieren: k-fold Cross Validation, dabei werden\n",
    "  * die Trainingsdaten in k Teile aufgeteilt,\n",
    "  * jeder Teil ist wird einmal zum Validieren verwendet, während die anderen k-1 Teile zum Trainieren verwendet werden\n",
    "  * so gibt es für jedes sample aus dem Trainingsset eine prediction\n",
    "  * über alle diese predictions wird dann der RMSE berechnet\n",
    "  * k liegt meist zwischen 3 und 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie stark unsere (zufällige) Aufteilung in 80% für das Training und 20% für die Validierung das Validierungsresultat beeinflusst, sehen mir mit k-Fold Cross Validation. Wir führen unsere Aufteilung in 80/20 fünf mal durch, so dass jedes Sample einmal im Validierungsset ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 folds\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(rf, pd.concat([X_train, X_test]),\n",
    "                         pd.concat([y_train, y_test]),\n",
    "                         cv=5, scoring='neg_mean_squared_error')\n",
    "np.sqrt(-1*scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie wir sehen, hat die Aufteilung in unserem Beispiel einen starken Einfluss auf unseren RMSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unter ensembling verseht man das Kombinieren mehrerer Modelle, um das Resultat zu verbessern. Wir kombinieren im Folgenden die Resultate unserer zwei Modelle, indem wir den gewichteten Durchschnitt beider Resultate nehmen (da Random Forests etwas besser abgeschnitten hat, geben wir ihm etwas mehr Gewicht).\n",
    "\n",
    "Es gibt viele weitere und komplexere Möglichkeiten, verschiedene Algorithmen zu kombinieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_error = rmse_lr + rmse_rf\n",
    "weight_lr = 1-rmse_lr/combined_error\n",
    "weight_rf = 1-rmse_rf/combined_error\n",
    "\n",
    "print(\"Lineare Regression:\\t {}\".format(rmse_lr))\n",
    "print(\"Random Forests:\\t\\t {}\".format(rmse_rf))\n",
    "print(\"Weighted Avgerage:\\t {}\".format(np.sqrt(mean_squared_error(y_test, weight_lr*pred_lr + weight_rf*pred_rf))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, das bringt nicht immer was..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun kann man das Modell bzw. den trainierten Algorithmus auf unbekannte Daten anwenden. Wir haben im Beispiel zwar keine solchen, tun aber so, als ob:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rf.predict(X_test)\n",
    "pd.DataFrame(list(zip(predictions[0:10], y_test[0:10])), columns=['y_hat', 'y'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
