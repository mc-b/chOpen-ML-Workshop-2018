{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einleitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Toy Projekt arbeiten wir mit Tweets von amerikanischen Politikern. Wir wollen einmal sehen, ob es möglich ist, den typischen Tweet-Style des amerikanischen Chefpolitikers mit Machine Learning zu erkennen und seine Tweets von anderen zu unterscheiden.\n",
    "\n",
    "Dafür verwenden wir den Cloud-Dienst [Monkeylearn](https://monkeylearn.com). Monkeylearn hat sich auf Natural Language Processing spezialisiert, und bietet eine Vielzahl an vorkonfigurierten Klassifiern, die man mit eigenen Daten trainieren und verwenden kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsere Daten holen wir aus dem [Trump Twitter Archive](http://www.trumptwitterarchive.com/). Direkte Downloads von Archiven sind [hier](https://github.com/bpb27/trump-tweet-archive) und weitere Archive sind [hier](https://github.com/bpb27/political_twitter_archive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einige Archive sind bereits im Docker Image drin:\n",
    "!ls /data/toyprojects/data/tweets/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zuerst laden wir die Daten von Donald Trump:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T05:47:04.299734Z",
     "start_time": "2018-09-06T05:47:04.170123Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/data/toyprojects/data/tweets/donald_trump/condensed_2016.json', 'r') as f:\n",
    "    trump_tweets = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun laden wir die Daten von Ben Carson, einem weiteren Republikaner. Wir wählen bewusst zwei Republikaner. Mit Trump und einem Demokraten wäre die Aufgabe wohl etwas zu einfach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-06T05:47:04.586593Z",
     "start_time": "2018-09-06T05:47:04.576093Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('/data/toyprojects/data/tweets/others/realbencarson_short.json', 'r') as f:\n",
    "    other_tweets = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schauen wir uns die Struktur mal an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tweets[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es interessieren uns lediglich `text` und `is_retweet`. Wir säubern nun die Daten ein wenig. Zum Beispiel entfernen wir Links, @mentions, Hashtags. Dies wiederum, um dem Algorithmus die Sache nicht zu einfach zu machen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def filter_and_clean_tweets(j):\n",
    "    \"\"\"Filter out retweets and clean remaining tweets\n",
    "    downloaded from http://www.trumptwitterarchive.com/ a bit\n",
    "    \"\"\"\n",
    "    \n",
    "    # filter out retweets and extract tweet text\n",
    "    tweets = [entry['text'] for entry in j if not entry['is_retweet']]\n",
    "\n",
    "    # replace \\n by space\n",
    "    tweets = [re.sub(r'\\n', ' ', tweet) for tweet in tweets]\n",
    "\n",
    "    # remove double quotes\n",
    "    tweets = [re.sub(r'\"', '', tweet) for tweet in tweets]\n",
    "\n",
    "    # remove leading dot\n",
    "    tweets = [re.sub(r'^\\.', '', tweet) for tweet in tweets]\n",
    "\n",
    "    # remove 'RT ' at beginning\n",
    "    tweets = [re.sub(r'^RT\\s*', '', tweet) for tweet in tweets] \n",
    "\n",
    "    # remove @mentions\n",
    "    tweets = [re.sub(r'@\\w+:?', r'', tweet) for tweet in tweets] \n",
    "\n",
    "    # remove hashtags\n",
    "    tweets = [re.sub(r'#\\w+:?', r'', tweet) for tweet in tweets]\n",
    "\n",
    "    # remove links (do it several times to catch them all)\n",
    "    for i in range(3):\n",
    "        tweets = [re.sub(r'(.*)\\s*https?://.+\\s*(.*)', r'\\1 \\2', tweet) for tweet in tweets]\n",
    "\n",
    "    # remove whitespace from beginning and end\n",
    "    tweets = [tweet.rstrip().lstrip() for tweet in tweets]\n",
    "\n",
    "    # replace &amp; with &\n",
    "    tweets = [re.sub(r'&amp;', r'&', tweet) for tweet in tweets]\n",
    "\n",
    "    # condense multiple spaces\n",
    "    tweets =[re.sub(r'\\s+', r' ', tweet) for tweet in tweets]\n",
    "\n",
    "    # return result for all tweets that are not empty now after the cleaning\n",
    "    return [tweet for tweet in tweets if tweet != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tweets = filter_and_clean_tweets(trump_tweets)\n",
    "other_tweets = filter_and_clean_tweets(other_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schauen wir uns das schnell an (mit irgendwelchen zufälligen Indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tweets[42:45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gut. Da der uns von MonkeyLearn freundlicherweise zur Verfügung gestellte Account die Modellgrösse auf 3000 Samples limitiert, beschneiden wir unser Datenset auf 3250 Samples. Davon werden wir die ersten 3000 für das Training verwenden, und die folgenden 250 für die Validierung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verwende die Methode `sample()` des Python Moduls [random](https://docs.python.org/3.7/library/random.html), um die beiden Listen trump_tweets und other_tweets auf eine Länge von je 1625 Samples zu kürzen. So balancieren wir das Datenset auch gleich aus.\n",
    "\n",
    "Neben der verlinkten Modulbeschreibung kannst das Python-interne Hilfesystem verwenden: Gib `help(random.sample)`. Manchmal funktioniert auch die Tab-Completion für Parameter nicht schlecht: `random.sample(<tab>...)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "trump_tweets = ...\n",
    "other_tweets = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "#### Vorschlag zur Umsetzung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "trump_tweets = random.sample(trump_tweets, 1625)\n",
    "other_tweets = random.sample(other_tweets, 1625)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, nun haben wir unsere Daten in Form von zwei Listen. Machen wir daraus einen Pandas DataFrame, damit können wir die Daten flexibel umformen (natürlich hätten wir auch gleich mit Pandas und mit `pd.read_json()` beginnen können)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erstelle einen DataFrame mit einem Tweet pro Zeile und mit zwei Spalten, eine Spalte mit dem tweet Text aller Trump und NotTrump tweets und eine mit dem Label: für Trump-Tweets 'Trump' und für die anderen 'Other'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle den Pandas DataFrame wie oben beschrieben\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(...)\n",
    "\n",
    "df.columns=['text', 'tags'] # verwende diese zwei Spaltennamen\n",
    "df = df.sample(frac=1) # mischen, dann können wir später einfach das Validierungsset hinten abschneiden\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "#### Vorschlag zur Umsetzung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Es gibt viele Möglichkeiten, wie man diesen DataFrame erstellen kann.\n",
    "# Hier eine kompakte, auch wenn vielleicht nicht die leserlichste\n",
    "# Das .T am schluss dreht den DataFrame (.T für transpose)\n",
    "df = pd.DataFrame([trump_tweets+other_tweets, ['Trump']*len(trump_tweets)+['Other']*len(other_tweets)]).T\n",
    "\n",
    "df.columns=['text', 'tags'] # verwende diese zwei Spaltennamen\n",
    "df = df.sample(frac=1) # mischen, dann können wir später einfach das Validierungsset hinten abschneiden\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun schauen wir uns das API von Monkeylearn an (via [offizieller Monkey Learn Python Client](https://github.com/monkeylearn/monkeylearn-python)). Hier ist die gesamte [API Referenz](https://monkeylearn.com/api/v3/#classifier-api)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir initialisieren das API und ertstellen ein Modell. Wähle im untenstehenden Code einen eigenen Classifier-Namen, möglichst so, dass die anderen Workshop-Teilnehmer nicht den gleichen Namen erwischen. Führe dann den Code aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classifier_name = \"...\" # benenne hier Deinen classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monkeylearn import MonkeyLearn\n",
    "\n",
    "API_KEY = ...\n",
    "\n",
    "# Erstelle ein ml Objekt\n",
    "ml = MonkeyLearn(API_KEY)\n",
    "\n",
    "# Erstelle einen Klassifier. ACHTUNG: Mit dem zur Verfügung stehenden API Key\n",
    "# können maximal 20 Classifier gleichzeitig erstellt werden!\n",
    "res = ml.classifiers.create(my_classifier_name)\n",
    "res.body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit dem Account, den wir verwenden, können genau 20 Classifiers erstellt werden. Bitte erstelle deshalb nur einen Classifier, damit die anderen Workshop-Teilnehmer auch einen machen können. Musst Du einen Classifier löschen, verwende das API oder melde Dich bei mir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schau Dir nun die oben verlinkte API Referenz an und vervollständige den folgenden Code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hohl die ID des neuen Models\n",
    "model_id = ...\n",
    "\n",
    "# Erstelle zwei neue Tags (Klassen) mit den Bezeichnern 'Trump' und 'Other'\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "#### Vorschlag zur Umsetzung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# Hohl die ID des neuen Models\n",
    "model_id = res.body['id']\n",
    "\n",
    "# Erstelle zwei neue Tags (Klassen) mit den Bezeichnern 'Trump' und 'Other'\n",
    "res = ml.classifiers.tags.create(model_id, 'Trump')\n",
    "res = ml.classifiers.tags.create(model_id, 'Other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes bereiten wir unsere Trainingsdaten vor.\n",
    "\n",
    "Das Classifier API benötigt eine Liste von Samples, worin jedes Element ein Dictionary mit zwei Keys ist. Für den Key 'text' geben wir als Value den entsprechenden Tweet als String. Für den Key 'tags' geben wir als Value eine Liste mit Länge 1, deren Element das entsprechende Label als String ist, also entweder 'Trump' oder 'Other'.\n",
    "\n",
    "Verwende nur die ersten 3000 Samples unseres Datarame: `df.iloc[0:3000]`, die restlichen 250 verwenden wir später zur Validierung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle eine Liste aus X_train und y_train\n",
    "\n",
    "train_samples = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "#### Vorschlag zur Umsetzung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# Auch hier gibt es viele Wege, zum Ziel zu kommen. Einer davon, mit der in Python gängigen list comprehension:\n",
    "train_samples = [{'text':d['text'], 'tags':[d['tags']]} for d in df.iloc[0:3000].to_dict(orient='records')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir die Trainingsdaten hochladen. MonkeyLearn started danach das Training automatisch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples uploaden\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "#### Vorschlag zur Umsetzung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# Samples uploaden\n",
    "ml.classifiers.upload_data(model_id, train_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Training dauert nicht lange. Sobald der Classifier im Feld last_trained ein Datum enthält, wurde er trainiert. Frage dieses Feld ab, und wiedehole die Abfrage (durch mehrmaliges, manuelles Auusführen der Notebook-Zelle), bis das Training beendet wurde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "#### Vorschlag zur Umsetzung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "ml.classifiers.detail(model_id).body['last_trained']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Nun validieren wir das Modell mit unserem Validierungsset. Schaue in der API-Dokumentation nach, in welcher Form der API-Call `classsify()` die zu klassifizierenden Samples haben möchte, forme um und führe den Call aus.\n",
    "\n",
    "**Achtung**: Da die verfügbaren Requests beschränkt sind, pass bitte auf, dass Du keine Endlosloops baust. Pro Teilnehmer stehen aber immerhin knapp 20'000 Requests für die Prediction zur Verfügung, ein bisschen Rumprobieren ist also erlaubt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = df....\n",
    "\n",
    "# Mache predictions für das ganze Validierungsset. Speichere sie vorerst so, wie sie von MonkeyLearn zurückkommen.\n",
    "predictions = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "#### Vorschlag zur Umsetzung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "test_samples = df.iloc[3000:].text.tolist()\n",
    "\n",
    "# Mache predictions für das ganze Validierungsset. Speichere sie vorerst so, wie sie von Monkeylearn zurückkommen.\n",
    "predictions = ml.classifiers.classify(model_id, test_samples).body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, eine Prediction sieht so aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ist etwas unpraktisch, extrahieren wir das. Du kannst der Einfachheit halber davon ausgehen, dass hier immer genau ein Tag zurückgegeben wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "#### Vorschlag zur Umsetzung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "pred = pd.DataFrame([(d['text'], d['classifications'][0]['tag_name']) for d in predictions], columns=['text', 'tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir kontrollieren noch kurz, ob die Reihenfolge unserer Testdaten mit derjenigen der Predictions immer noch übereinstimmt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[3000:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das scheint zu passen. Nun berechnen wir die Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(df.iloc[3000:].tags, pred.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicht schlecht! Mehr als 80% der Samples wurden korrekt klassifiziert. Damit wären wir am Ende dieses Toy Projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Wenn Du möchtest, kannst nun etwas weiter experimentieren. Zum Beispiel einige Settings des Modells anpassen und neu trainieren und validieren:\n",
    " * Anstatt den Algorithmus Naive Bayes (nb) eine Support Vector Machine (svm) verwenden\n",
    " * max_features veroppeln\n",
    " * Unigrams und Bigrams verwenden (ngram_range -> [1, 2])\n",
    " * Unser primitives Preprocessing vom Anfang des Notebooks weglassen und dafür Preprocessing für Social Media einschalten\n",
    " * Stemming ausschalten\n",
    " \n",
    "Oder wertest eine andere Metric als Accuracy aus, beispielsweise Area Under the ROC Curve oder Logarithmic Loss. Dazu benötigst Du nicht nur die Prediction, sondern auch die Confidence.\n",
    "\n",
    "Aufschlussreich ist es beispielsweise auch, sich einmal das Dashboard von [Monkeylearn](https://monkeylearn.com) anzuschauen. Dazu müsstest Du Dich aber selber dort registrieren. Für den Free Tier braucht es lediglich eine Email-Adresse, damit kannst Du Modelle mit maximal 3000 Samples trainieren und pro Monat 1000 Requests machen. Trainieren zählt nicht zu den Requests.\n",
    "Du kannst auch auf den Team Tier upgraden, dann bekommst Du 300'000 Requests. Dafür benötigst Du aber eine Kreditkarte, und nach 14 Tagen wird diese belastet, wenn Du vorher nicht kündigst.\n",
    "Alternativ kannst Du das Dashboard und Deinen Klassifier auch kurz auf meinem Laptop anschauen.\n",
    "\n",
    "Weitere Ideen:\n",
    "* Ein paar neuere Trump tweets klassifizieren und schauen, ob unser Detektor diese auch als von Trump stammend erkennt. Tweets von 2017 sind im Image vorhanden.\n",
    "* Tweets von jemand anderes als Trump und Ben Carson klassifizieren und schauen, ob unser Detektor diese auch als Nicht-Trump-Tweets erkennt\n",
    "* Hillary Clinton hinzunehmen und zwischen allen dreien unterscheiden. Tip: Das Modell muss nicht neu erstellt werden, es reicht, nur die neuen Tweets von Hillary hinzuzufügen und nochmals zu trainieren. Dann aber überprüfen, ob bei den predictions nun jeweils mehr als ein Tag zurückkommt.\n",
    "\n",
    "Und zuletzt noch dies: Wie ich vergangene Woche [zu lesen war](http://varianceexplained.org/r/trump-tweets/), stammen Trump Tweets nicht nur von Trump himself, sondern auch von seinem Staff. Es wäre interessant, diese Analyse nachzuvollziehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wichtig**: Wenn Du Deinen Classifier nicht mehr brauchst, so lösche ihn doch bitte gleich:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.classifiers.delete(model_id)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
